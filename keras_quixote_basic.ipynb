{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from text_processing import quixote_data_cleaning, text_processing\n",
    "from RNN_class import Basic_RNN, Basic_RNN_v2, GRU_RNN, LSTM_RNN, OneStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to demonsrate how the data is imported, cleaned, processed and used to train and evaluate a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the data importing and cleaning function\n",
    "text=quixote_data_cleaning()\n",
    "\n",
    "sequence_length=100\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "#run the data processing function, takes a string and returns a tf.data object for efficent implementation in an RNN\n",
    "dataset,ids_from_chars, chars_from_ids, vocab_size=text_processing(text,sequence_length,BATCH_SIZE,BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing a basic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 74ms/step - accuracy: 0.3677 - loss: 2.3328\n",
      "Epoch 2/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 80ms/step - accuracy: 0.5040 - loss: 1.6679\n",
      "Epoch 3/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 83ms/step - accuracy: 0.5325 - loss: 1.5651\n",
      "Epoch 4/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 92ms/step - accuracy: 0.5460 - loss: 1.5154\n",
      "Epoch 5/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 97ms/step - accuracy: 0.5545 - loss: 1.4862\n",
      "Epoch 6/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 110ms/step - accuracy: 0.5601 - loss: 1.4639\n",
      "Epoch 7/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 121ms/step - accuracy: 0.5640 - loss: 1.4498\n",
      "Epoch 8/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 99ms/step - accuracy: 0.5669 - loss: 1.4396\n",
      "Epoch 9/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 103ms/step - accuracy: 0.5697 - loss: 1.4318\n",
      "Epoch 10/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 121ms/step - accuracy: 0.5722 - loss: 1.4212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x168ae9c10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim=256\n",
    "hidden_units=128\n",
    "\n",
    "basic_model = Basic_RNN(hidden_units, vocab_size,embedding_dim)\n",
    "EPOCHS=10\n",
    "basic_model.fit(dataset,epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'Don Quixote plind he athe an sthe malal ithist hand an a, s ha t cil and alllut at an all mand tind ter thaso Se'], shape=(1,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "temperature=0.6\n",
    "one_step_model = OneStep(basic_model, chars_from_ids, ids_from_chars, temperature)\n",
    "\n",
    "# give it an input of 'Don Quixote '\n",
    "next_char = tf.constant(['Don Quixote '])\n",
    "result = [next_char]\n",
    "\n",
    "#predict the next 100 characters\n",
    "for n in range(100):\n",
    "  next_char = one_step_model.generate_one_step(next_char)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very bad model but that is to be expected as it only took five minutes to train. \n",
    "It seems to understand how spaces and commas work but cannot produce words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
